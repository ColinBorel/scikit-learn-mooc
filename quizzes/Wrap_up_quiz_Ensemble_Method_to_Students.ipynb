{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZT5rCkgCJr9gRwOaMF3/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilals/scikit-learn-mooc/blob/main/quizzes/Wrap_up_quiz_Ensemble_Method_to_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap-up quiz - Ensemble Method\n",
        "\n",
        "**This quiz requires some programming to be answered.**\n",
        "\n",
        "This wrap-up quiz uses the penguins dataset, but notice that **we do not use the\n",
        "traditional `Species` column** as predictive target:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/bilals/scikit-learn-mooc/main/datasets/penguins.csv\")\n",
        "\n",
        "feature_names = [\n",
        "    \"Culmen Length (mm)\",\n",
        "    \"Culmen Depth (mm)\",\n",
        "    \"Flipper Length (mm)\",\n",
        "]\n",
        "target_name = \"Body Mass (g)\"\n",
        "\n",
        "dataset = dataset[feature_names + [target_name]].dropna(axis=\"rows\", how=\"any\")\n",
        "dataset = dataset.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "data, target = dataset[feature_names], dataset[target_name]\n",
        "```\n",
        "\n",
        "We therefore define our problem as a regression problem: we want to predict the\n",
        "body mass of a penguin given its culmen and flipper measurements.\n",
        "\n",
        "Notice that we randomly shuffled the rows of the dataset after loading it\n",
        "(`dataset.sample(frac=1, random_state=0)`). The reason is to break a spurious\n",
        "order-related statistical dependency that would otherwise cause trouble with the\n",
        "naive cross-validation procedure we use in this notebook. The problem of\n",
        "order-dependent samples will be discussed more in detail on the model evaluation\n",
        "module and is outside of the scope of this quiz for now.\n",
        "Now, evaluate the following tree-based models:\n",
        "\n",
        "- a decision tree regressor, i.e. `sklearn.tree.DecisionTreeRegressor`\n",
        "- a random forest regressor, i.e. `sklearn.ensemble.RandomForestRegressor`\n",
        "\n",
        "Use the default hyper-parameter settings for both models. The only exception\n",
        "is to pass `random_state=0` for all models to be sure to recover the exact\n",
        "same performance scores as the solutions to this quiz.\n",
        "\n",
        "Evaluate the generalization performance of these models using a 10-fold\n",
        "cross-validation:\n",
        "\n",
        "- use `sklearn.model_selection.cross_validate` to run the cross-validation routine\n",
        "- set the parameter `cv=10` to use a 10-fold cross-validation strategy. Store the\n",
        "training score of the cross-validation by setting the parameter\n",
        "`return_train_score=True` in the function `cross_validate`\n",
        "as we will use it later on.\n",
        "\n",
        "## Question 1\n",
        "```\n",
        "By comparing the cross-validation test scores fold-to-fold, count the number of times\n",
        "a random forest is better than a single decision tree.\n",
        "Select the range which this number belongs to:\n",
        "\n",
        "- a) [0, 3]: the random forest model is substantially worse than the single decision tree regressor\n",
        "- b) [4, 6]: both models are almost equivalent\n",
        "- c) [7, 10]: the random forest model is substantially better than the single decision tree regressor\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "\n",
        "Now, train and evaluate with the same cross-validation strategy a random forest\n",
        "with 5 decision trees and another containing 100 decision trees. Once again\n",
        "store the training score.\n",
        "\n",
        "## Question 2\n",
        "```\n",
        "By comparing the cross-validation test scores fold-to-fold, count the number of times\n",
        "a random forest with 100 decision trees is better than a random forest with\n",
        "5 decision trees.\n",
        "Select the range which this number belongs to:\n",
        "\n",
        "- a) [0, 3]: the random forest model with 100 decision trees is substantially worse than the random forest model with 5 decision trees\n",
        "- b) [4, 6]: both models are almost equivalent\n",
        "- c) [7, 10]: the random forest model with 100 decision trees is substantially better than the random forest model with 5 decision trees\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "Plot the validation curve of the `n_estimators` parameters defined by:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_estimators = np.array([1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000])\n",
        "```\n",
        "\n",
        "## Question 3\n",
        "\n",
        "```\n",
        "Select the correct statements below.\n",
        "\n",
        "- a) the **train score** decreases when `n_estimators` become large (above 500 trees)\n",
        "- b) the **train score** reaches a plateau when `n_estimators` become large (above 500 trees)\n",
        "- c) the **train score** increases when `n_estimators` become large (above 500 trees)\n",
        "- d) the **test score** decreases when `n_estimators` become large (above 500 trees)\n",
        "- e) the **test score** reaches a plateau when `n_estimators` become large (above 500 trees)\n",
        "- f) the **test score** increases when `n_estimators` become large (above 500 trees)\n",
        "\n",
        "_Select all answers that apply_\n",
        "```\n",
        "\n",
        "Repeat the previous experiment but this time, instead of choosing the default\n",
        "parameters for the random forest, set the parameter `max_depth=5` and build\n",
        "the validation curve.\n",
        "\n",
        "## Question 4\n",
        "```\n",
        "Comparing the validation curve (train and test scores) of the random forest\n",
        "with a full depth and the random forest with a limited depth, select the correct\n",
        "statements.\n",
        "\n",
        "- a) the **test score** of the random forest with a full depth is (almost) always better than the **test score** of the random forest with a limited depth\n",
        "- b) the **train score** of the random forest with a full depth is (almost) always better than the **train score** of the random forest with a limited depth\n",
        "- c) the gap between the train and test scores decreases when reducing the depth of the trees of the random forest\n",
        "- d) the gap between the train and test scores increases when reducing the depth of the trees of the random forest\n",
        "\n",
        "\n",
        "_Select all answers that apply_\n",
        "```\n",
        "\n",
        "\n",
        "Let us now focus at the very beginning of the validation curves, and\n",
        "consider the training score of a random forests with a single tree\n",
        "while using the default `max_depth=None` parameter setting:\n",
        "\n",
        "```python\n",
        "rf_1_tree = RandomForestRegressor(n_estimators=1, random_state=0)\n",
        "cv_results_tree = cross_validate(\n",
        "    rf_1_tree, data, target, cv=10, return_train_score=True\n",
        ")\n",
        "cv_results_tree[\"train_score\"]\n",
        "```\n",
        "\n",
        "should return:\n",
        "\n",
        "```\n",
        "array([0.83120264, 0.83309064, 0.83195043, 0.84834224, 0.85790323,\n",
        "       0.86235297, 0.84791111, 0.85183089, 0.82241954, 0.85045978])\n",
        "\n",
        "```\n",
        "\n",
        "The fact that this single-tree Random Forest can never reach\n",
        "a perfect R2 score of 1.0 on the training can be surprising.\n",
        "\n",
        "Indeed, if you we evaluate the training accuracy of the single\n",
        "`DecisionTreeRegressor` one gets perfect memorization of the\n",
        "training data:\n",
        "\n",
        "```python\n",
        "tree = DecisionTreeRegressor(random_state=0)\n",
        "cv_results_tree = cross_validate(\n",
        "    tree, data, target, cv=10, return_train_score=True\n",
        ")\n",
        "cv_results_tree[\"train_score\"]\n",
        "```\n",
        "\n",
        "which outputs the expected perfect score:\n",
        "\n",
        "```\n",
        "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
        "```\n",
        "\n",
        "## Question 5\n",
        "```\n",
        "From the following statements, select the one that explains\n",
        "that a single-tree random forest cannot achieve perfect\n",
        "training scores.\n",
        "\n",
        "- a) the single tree in the random forest is trained using a bootstrap of the training set and not the training set itself (because `bootstrap=True` by default)\n",
        "- b) for a given feature, the single tree in the random forest uses random splits while the single decision tree uses the best split\n",
        "- c) the random forest automatically limits the depth of the single decision tree, which prevents overfitting\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "Build a validation curve for a `sklearn.ensemble.HistGradientBoostingRegressor`\n",
        "varying `max_iter` as follows:\n",
        "\n",
        "```python\n",
        "max_iters = np.array([1, 2, 5, 10, 20, 50, 100, 200, 500])\n",
        "```\n",
        "\n",
        "We recall that `max_iter` corresponds to the number of trees in the boosted\n",
        "model.\n",
        "\n",
        "Plot the average train and test score for each value of `max_iter`.\n",
        "\n",
        "## Question 6\n",
        "```\n",
        "Select the correct statements.\n",
        "\n",
        "- a) for a small number of trees (between 5 and 10 trees), the gradient boosting model behave like the random forest algorithm: the train scores are high while the test scores are not optimum\n",
        "- b) for a small number of trees (between 5 and 10 trees), the gradient boosting model behave differently to the random forest algorithm: both the train and test scores are small\n",
        "- c) with a large number of trees (> 100 trees) adding more trees in the ensemble causes the gradient boosting model overfit (increasing the gap between the train score and  test score)\n",
        "- d) with a large number of trees (> 100 trees) adding more trees in the ensemble does not impact the generalization performance of the gradient boosting model\n",
        "\n",
        "_Select all answers that apply_\n",
        "```"
      ],
      "metadata": {
        "id": "8ysaUIsCfGEG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXSxETPdfuxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}