{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kHR24SrrnTjOwzwSCeJrhH-v_W8_Wumh",
      "authorship_tag": "ABX9TyNq5jZ/IR49109nhbPn+Ccp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilals/scikit-learn-mooc/blob/main/quizzes/Wrap_Up_Quiz_Decision_Trees_to_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap-up quiz - Decision Trees\n",
        "\n",
        "**This quiz requires some programming to be answered.**\n",
        "\n",
        "Open the dataset `ames_housing_no_missing.csv` with the following command:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ames_housing = pd.read_csv(\"https://raw.githubusercontent.com/bilals/scikit-learn-mooc/main/datasets/ames_housing_no_missing.csv\")\n",
        "target_name = \"SalePrice\"\n",
        "data = ames_housing.drop(columns=target_name)\n",
        "target = ames_housing[target_name]\n",
        "```\n",
        "\n",
        "`ames_housing` is a pandas dataframe. The column \"SalePrice\" contains the\n",
        "target variable.\n",
        "\n",
        "To simplify this exercise, we will only used the numerical features defined\n",
        "below:\n",
        "\n",
        "```python\n",
        "numerical_features = [\n",
        "    \"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\",\n",
        "    \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
        "    \"GrLivArea\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\",\n",
        "    \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
        "    \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\",\n",
        "]\n",
        "\n",
        "data_numerical = data[numerical_features]\n",
        "```\n",
        "\n",
        "We will compare the generalization performance of a decision tree and a linear\n",
        "regression. For this purpose, we will create two separate predictive models\n",
        "and evaluate them by 10-fold cross-validation.\n",
        "\n",
        "Thus, use `sklearn.linear_model.LinearRegression` and\n",
        "`sklearn.tree.DecisionTreeRegressor` to create the models. Use the default\n",
        "parameters for the linear regression and set `random_state=0` for the decision\n",
        "tree.\n",
        "\n",
        "Be aware that a linear model requires to scale numerical features.\n",
        "Please use `sklearn.preprocessing.StandardScaler` so that your\n",
        "linear regression model behaves the same way as the quiz author\n",
        "intended ;)\n",
        "\n",
        "## Question 1\n",
        "```\n",
        "By comparing the cross-validation test scores for both models fold-to-fold, count the number of\n",
        "times the linear model has a better test score than the decision tree model.\n",
        "Select the range which this number belongs to:\n",
        "\n",
        "- a) [0, 3]: the linear model is substantially worse than the decision tree\n",
        "- b) [4, 6]: both models are almost equivalent\n",
        "- c) [7, 10]: the linear model is substantially better than the decision tree\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "\n",
        "Instead of using the default parameters for the decision tree regressor, we will\n",
        "optimize the `max_depth` of the tree. Vary the `max_depth` from 1 level up to 15\n",
        "levels. Use nested cross-validation to evaluate a grid-search\n",
        "(`sklearn.model_selection.GridSearchCV`). Set `cv=10` for both the inner and\n",
        "outer cross-validations, then answer the questions below.\n",
        "\n",
        "## Question 2\n",
        "```\n",
        "What is the optimal tree depth for the current problem?\n",
        "\n",
        "- a) The optimal depth is ranging from 3 to 5\n",
        "- b) The optimal depth is ranging from 5 to 8\n",
        "- c) The optimal depth is ranging from 8 to 11\n",
        "- d) The optimal depth is ranging from 11 to 15\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "Now, we want to evaluate the generalization performance of the decision tree\n",
        "while taking into account the fact that we tune the depth for this specific\n",
        "dataset. Use the grid-search as an estimator inside a `cross_validate` to\n",
        "automatically tune the `max_depth` parameter on each cross-validation\n",
        "fold.\n",
        "\n",
        "## Question 3\n",
        "\n",
        "```\n",
        "A tree with tuned depth\n",
        "\n",
        "- a) is always worse than the linear models on all CV folds\n",
        "- b) is often but not always worse than the linear model\n",
        "- c) is often but not always better than the linear model\n",
        "- d) is always better than the linear models on all CV folds\n",
        "\n",
        "_Select a single answer_\n",
        "\n",
        "Note: Try to set the random_state of the decision tree to different values\n",
        "e.g. random_state=1 or random_state=2 and re-run the nested cross-validation\n",
        "to check that your answer is stable enough.\n",
        "```\n",
        "\n",
        "\n",
        "Instead of using only the numerical features you will now use the entire dataset\n",
        "available in the variable `data`.\n",
        "\n",
        "Create a preprocessor by dealing separately with the numerical and categorical\n",
        "columns. For the sake of simplicity, we will assume the following:\n",
        "\n",
        "- categorical columns can be selected if they have an `object` data type;\n",
        "- use an `OrdinalEncoder` to encode the categorical columns;\n",
        "- numerical columns should correspond to the `numerical_features` as defined above.\n",
        "  This is a subset of the features that are not an `object` data type.\n",
        "\n",
        "In addition, set the `max_depth` of the decision tree to `7` (fixed, no need\n",
        "to tune it with a grid-search).\n",
        "\n",
        "Evaluate this model using `cross_validate` as in the previous questions.\n",
        "\n",
        "## Question 4\n",
        "```\n",
        "A tree model trained with both numerical and categorical features\n",
        "\n",
        "- a) is most often worse than the tree model using only the numerical features\n",
        "- b) is most often better than the tree model using only the numerical features\n",
        "\n",
        "_Select a single answer_\n",
        "\n",
        "Note: Try to set the random_state of the decision tree to different values\n",
        "e.g. random_state=1 or random_state=2 and re-run the (this time single) cross-validation\n",
        "to check that your answer is stable enough.\n",
        "```\n"
      ],
      "metadata": {
        "id": "kF4UZvosZJMK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSQ3nH_ceHHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}